---
title: ""
output:
  bookdown::pdf_document2:
    fig_crop: no
    fig_caption: yes
    toc: no
    number_sections: TRUE
    latex_engine: xelatex
  word_document: default
keep_tex: yes
header-includes:
- \usepackage[UKenglish]{isodate}
- \cleanlookdateon
- \usepackage{natbib}
- \bibliographystyle{agsm}
- \usepackage{hyperref}
- \usepackage{float}
- \usepackage[hang,flushmargin, bottom]{footmisc}
- \usepackage{sectsty}
- \usepackage{setspace}
- \usepackage{geometry}
- \usepackage{graphicx}
- \usepackage{fvextra}
- \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
- \newcommand\cites[1]{\citeauthor{#1}'s\ (\citeyear{#1})}


bibliography: references.bib
---

\newgeometry{top=1.8in,bottom=1.8in,right=1in,left=1in}
\spacing{1.7}

\allsectionsfont{\centering}
\subsectionfont{\raggedright}
\subsubsectionfont{\raggedright}

\pagenumbering{gobble}

\begin{centering}
  \newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
	\includegraphics[width=0.4\textwidth]{kuleuven_logo.png}\\[1cm]
	\textsc{\Large Collecting and Analyzing Big Data \\ for Social Sciences}\\[0.5cm]
	\textsc{\large B-KUL-S0K17A}\\[0.5cm]
	\HRule\\[0.6cm]
	{\huge\bfseries Assignment 2:} \\ [0.25cm]
	{\LARGE\bfseries Research Notebook}\\[0.2cm]
	\HRule\\[1.5cm]
	{\Large\textit{Group 24:}}\\[0.25cm]
	\large Ekaterina \textsc{Zaitseva} - r0870363\\
	\large Emma \textsc{Schweidler} - r0883088 \\
	\large Jade \textsc{Willaert} - r0762578\\
	\vfill\vfill\vfill
	{\large\today}
	
\end{centering}

\allsectionsfont{\raggedright}

\singlespacing

 
\newpage

\newgeometry{top=1in,bottom=1in,right=1in,left=1in}
\pagenumbering{arabic} 

```{r setup, include=FALSE}
# add space between text and code chunks
hook_source_def = knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  if (!is.null(options$vspaceecho)) {
    begin <- paste0("\\vspace{", options$vspaceecho, "}")
    stringr::str_c(begin, hook_source_def(x, options))
  } else {
    hook_source_def(x, options)
  }
})

hook_output_def = knitr::knit_hooks$get('output')
knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(options$vspaceout)) {
    end <- paste0("\\vspace{", options$vspaceout, "}")
    stringr::str_c(hook_output_def(x, options), end)
  } else {
    hook_output_def(x, options)
  }
})

# global settings
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, out.width="80%", fig.align="center",  fig.pos="H", vspaceecho='1em', vspaceout='1em')

# formatting
library(knitr)
library(formatR)
cutoff = 80
opts_chunk$set(tidy.opts = list(width.cutoff = cutoff), tidy = TRUE)
usage(head, width = cutoff)


```


<!-- Make sure following aspects are included: short problem statement & research question (without extensive literature review), methodological design, description of data (collection), data cleaning/wrangling steps, analysis, robustness checks (if applicable), results, limitations, conclusion, pitch -->

# Research Question

In March of this year, the Youtube channel \citet{JaidenAnimationsVideo} uploaded a video titled ‘Being Not Straight’. The animated story, which has since received almost 12 million views, is a coming out video that introduces the audience to aromanticism and asexuality. As both of these orientations are still relatively unknown, our project seeks to analyse the comment section under the video so as to investigate how the video was received. More precisely, we are interested in the audience's sentiment and the main topics discussed in the comments.

# Methodological Design

brief outline and justification of our research design, ideally supported by scientific literature

# Data

## Data Collection

import packages

```{r packages, message=F}
# import packages
library(tidyverse)
library(tuber)
library(quanteda) 
library(quanteda.textstats)
library(quanteda.textplots)
library(tm)
library(stopwords)
library(lexicon)
library(Matrix)
library(vader)
```

First, we need to scrap the comments via the Youtube API. In R, there are two packages to pursue this: `vosonSML` and `tuber`. 

There are reportedly issues with the `tuber` package \citep{IssuesTuber} and nested comments in general \citep{IssuesNestedComments} 

At the time of data collection (i.e., 16th May 2022), the video had 231,866 comments. When scraping the comments under the video with both packages, we received the following results:

\begin{itemize}
  \item `vosonSML`: 151,884 scraped comments
  \item `tuber`: 180,743 scraped comments
\end{itemize}

For comparability purposes, we also used \cites{RiederTools} php tool which resulted in 218,848 scraped comments.
  
It is noteworthy that none of the tested packages and tools were able to extract all Youtube comments. Since we conduct our analysis in R, we opted for the `tuber` package as this package parsed about 30,000 comments more than the `vosonSML` package.

Code used to scrape comments (data were collected on 16th May 2022):

```{r tuber, eval = F}
# authenticate Youtube API
# (for security reasons, our personal APP_ID and APP_SECRET are not included in this script)
yt_oauth("APP_ID", "APP_SECRET", token = '')

# get statistics about the video
videoid <- "qF1DTK4U1AM"
get_stats(video_id = videoid)

# scrape comments
comments <- get_all_comments(video_id = videoid)

# write output to file
write_csv(comments, "being_not_straight_16_may_22_tuber.csv")  
```

## Data Wrangling

```{r data import}
# import data
comments <- read.csv(unzip("being_not_straight_16_may_22_tuber.csv.zip","being_not_straight_16_may_22_tuber.csv"), header = T)
```


### Data Frame Cleaning

```{r variable names}
# get variable names
print(names(comments))
```


As suggested by \citet{YoutubeGenderSentiment}, any comments written by the content creator should be removed before embarking on the data analysis

```{r remove creator comment}
# remove comments posted by Jaiden Animations
comments <- comments %>%
  filter(!authorChannelId.value == 'UCGwu0nbY2wSkW8N-cghnLpA')
```


```{r boolean thread}
# create Boolean variable for thread comments
comments <- comments %>% 
  mutate(ThreadComment = if_else(is.na(parentId), FALSE, TRUE)) 
```

<!-- I decided to include the information about the variable moderationStatus because if there were any values there, it would be important to us. But the fact that we checked it out and found nothing is still valuable, so I would keep it. -->

According to \citet{YoutubeDocumentation}, valid values for the variable \emph{moderationStatus} are heldForReview, likelySpam, published, and rejected. We will therefore first check what values are in this variable and delete comments that are marked as spam or have been rejected, if there are any.

```{r moderationStatus}
# check values of variable moderationStatus
comments %>% count(moderationStatus)
```

Since the field \textit{moderationStatus} is empty, we will not proceed with this column.

Initially, we have 15 variables which are described in the official \citet{YoutubeDocumentation} documentation. 

In our analysis, we will focus on the following properties of the video:
<!-- the list was paraphrased -->
\begin{itemize}
  \item \textit{textOriginal}: unprocessed commentary text as originally published or last updated
  \item \textit{likeCount}: number of likes on the comment
  \item \textit{parentId}: unique identifier of the parent comment, only filled in if the comment is a response to another comment
  \item \textit{id}: unique identifier of the comment
\end{itemize}

Between the \textit{textDisplay} and \textit{textOriginal} variables, \textit{textOriginal} was chosen because \textit{textDisplay} is the comment's text which was retrieved in html format. Thus, for example, in  \textit{textDisplay}, video links may be replaced with video titles and we can count them as words the user wrote in the video or the emoji can be converted to text. Other variables, such as the publication date or the date of comment update, will be removed from the data frame as our main focus is on what people posted and how they responded. Moreover, we are aware that since we are working with social media comments, we have to take ethics into account - that's why we removed all usernames and user characteristics, thereby anonymising the comments. 

```{r subset columns}
# select columns
comments.cut <- comments %>% select(c(id, textOriginal, likeCount, ThreadComment))
```

Next, we filter the comments by their thread status. By only retaining parent comments, the comments that were not scraped by the `tuber` package - which are most likely only comment replies due to issues with collecting nested comments with the Youtube API - do not affect our research.


```{r separate data frame in parent-reply}
# filter parent comments
og.comments <- comments.cut %>% 
       filter(ThreadComment==FALSE)
```

<!-- Do we really need to compute the proportions of urls, taggings and comments (as in the chunk below)? I don't think it's related to our research aims -->

<!-- Counting the proportion of urls, taggings and hashtags in the comments is just to check if they are there at all, and to understand exactly how much text we will remove. It could be that people are using a lot of hashtags, then we would need to look in more detail to see if they are all worth deleting. This is part of the exploratory analysis. -->

```{r hashtag-urls-taggings}
# get number of hashtags
hashtags.n <- sum(str_detect(og.comments$textOriginal, "#\\w+"))

# get number or urls
urls.n <- sum(str_detect(og.comments$textOriginal, "(http|ftp|https)://\\S+\\s*"))

# get number of taggings
mentions.n <- sum(str_detect(og.comments$textOriginal, "@\\w+"))

# get number of tweets with non-ASCII characters - emojis and non-English words
nonengl.n <- sum(str_detect(og.comments$textOriginal, "[^\x01-\x7F]"))

# get number of comments
n <- dim(og.comments)[1]

# display results
cat("Share of parent comments with hashtags:", round(hashtags.n/n,3), 
    "\nShare of parent comments with urls:", round(urls.n/n,3), 
    "\nShare of parent comments with taggings:", round(mentions.n/n,3),
    "\nShare of parent comments with words in non-English languages:", round(nonengl.n/n,3))
```


### Noise Removal

The next step is to clean the column \textit{textOriginal} as this is the main variable we will be working with.

Compared to the html format of the comments (i.e., \emph{textDisplay}), not much data cleaning is needed in the original text. Nonetheless, we still have to remove noise such as quotation marks, line breaks, taggings, hashtags and non-English words.

```{r noise removal}
# code adapted from lecture 4 part 1

textCleaned <- og.comments$textOriginal %>% 
  # remove backslashes
  str_replace_all("\"", "") %>%
  # remove line breaks
  str_replace_all("\n", " ") %>%
  # remove at-mentions and hashtags
  str_replace_all("(@|#)\\S+", " ") %>% 
  # remove urls
  str_replace_all("(http|ftp|https)://\\S+\\s*", " ") %>%
  # remove numbers
  str_replace_all("[0-9]+", " ") %>%       
  #remove common non-ASCII characters
  str_replace_all("[^\x01-\x7F]", " ") %>%    
  # lower case
  tolower() %>% 
  # remove multiple whitespace characters
  str_replace_all("\\p{space}+", " ") %>%     
  # trim whitespace characters at start and end
  str_remove_all("^\\s+|\\s+$") 

# insert cleaned comments in a column after the original comments
cleaned <- og.comments %>%
  mutate(textCleaned = textCleaned,
        .after = textOriginal)
```


Remove rows which contained only the characters we got rid of

```{r remove empty rows}
cleaned <- cleaned[cleaned$textCleaned!="",]
```

resource for stopwords (and data cleaning in general): https://smltar.com/stopwords.html

<!-- Is there a stopword list tailored to social media data? 

I didn't find
-->

### Tokenisation

```{r tokenisation}
# tokenise; remove stopwords, special characters and words with less than 3 characters
comments_token <- corpus(cleaned$textCleaned) %>% 
  tokens() %>%
  tokens_remove(stopwords(source = 'smart')) %>%
  tokens_keep("^\\p{L}", valuetype = 'regex') %>%
  tokens_keep(min_nchar = 3)

# show tokens of second comment
comments_token[["text2"]]
```

### Lemmatisation

We normalise the comments by means of lemmatisation

```{r lemmatisation}
# lemmatisation
comments_lemma <- comments_token %>%
  tokens_replace(pattern = hash_lemmas$token, replacement = hash_lemmas$lemma)

# show lemmatised tokens of second comment
comments_lemma[["text2"]]
```

## Data Exploration

### High Frequency Words

```{r list freq words}
# get high frequency words
comments_freq <- comments_lemma %>%  
  dfm() %>%
  textstat_frequency() 

# show ten most frequent words
comments_freq %>%
  head(10)
```

We can see in the Figure \@ref(fig:plot-frequency) that the word frequencies are unequally distributed. In the left plot, it is apparent that the 500 most frequent words account for about 95% of our data set. In the right plot, we find that over 10,000 words appear only once in all the comments. There are also seemingly no words that appear exactly seven, nine or eleven times.

```{r plot-frequency, fig.cap="Word Distribution Frequencies"}
# code for plot adapted from https://rpubs.com/wjnaramore/723341

# plot cumulative distribution frequency (CDF)
par(mfrow = c(1,2))
plot(cumsum(comments_freq$frequency)/sum(comments_freq$frequency),
     type = "l", xlab = "Number of Words", ylab = "Cumulative Distribution Frequency", 
     main = "Word CDF", col = 'red',
     panel.first = grid())

# plot histogram of frequencies
comments_hist <- hist(comments_freq$frequency, plot = FALSE)
plot(comments_hist$counts, ylab = "Frequency", xlab = "Word Frequency",
     main = "Word Frequency", log = "y", type = "h", col = 'red')
par(mfrow = c(1,1))
```

To identify words that are not meaningful, we went through the 100 most frequent words (output omitted due to its length). Upon identification, the words were removed from the corpus.

```{r remove freq word}
# add words to stopword list
my_stopwords <- stopwords('english', source = 'smart')
my_stopwords <- c(my_stopwords, c('feel', 'people', 'video', 'jaiden', 'make', 'thing', 
                                  'lot', 'year', 'part', 'stuff', 'do', 'kinda', 'yes', 'back'))
  
# remove stopwords
comments_stopw <- comments_lemma %>% 
  tokens_remove(my_stopwords) %>%
  dfm()

head(comments_stopw)
```

Since we analyse each individual comment, it was decided not to remove low frequency words at this stage.

<!-- good example [of data cleaning]
https://gist.github.com/CateGitau/05e6ff80b2a3aaa58236067811cee44e --> 

Figure \@ref(fig:word-cloud) visualises the word frequencies in form of a word cloud. 

```{r word-cloud, fig.cap="Word Cloud"}
# print word cloud, code is adapted from Lecture 4 (Part 2)
colors = RColorBrewer::brewer.pal(8, "Accent")
textplot_wordcloud(comments_stopw, max_words=200, 
    #regulate size of the samllest and largest words               
    min_size = 1, max_size=4, 
    # words will be plotted in decreasing freq
    random_order=FALSE,
    # colors will be assigned based on freq
    random_color= FALSE, color=colors)
```

```{r build n-grams}
# build bigrams from tokens after lemmatisation
comments_ngram <- comments_lemma %>% 
  tokens_remove(my_stopwords) %>% 
  tokens_ngrams(2, concatenator = " ") %>% 
  dfm()

head(comments_ngram)
```

```{r frequency of n-grams}
# get frequency of ngrams
comments_ngram_freq <- comments_ngram %>%
  textstat_frequency() 

# show ten most frequent words
# part of code is taken from https://quanteda.io/reference/textstat_frequency.html
ggplot(comments_ngram_freq[1:10, ], aes(x = reorder(feature, frequency), y = frequency)) +
    geom_point() +
    coord_flip() +
    labs(x = NULL, y = "Frequency")
```
We may notice that bigram has the problem that they may be the same in terms of word combinations, but the words are in different places, so they count as different (like 'aro ace' and 'ace aro'). We could not find a built-in function in R to eliminate this problem, so an algorithm was written.

```{r sum reversed bigrams}
#split bigrams, sort words within each bigram and collapse them together 
colnames(comments_ngram) <- sapply(strsplit(colnames(comments_ngram), " "), function(x) paste(sort(x), collapse=" "))
#the code for sparseMatrix to combine columns with identical names was adapted from https://stackoverflow.com/questions/36778166/how-to-combine-columns-with-identical-names-in-a-large-sparse-matrix
nms <- colnames(comments_ngram)
uniquenms <- unique(nms)

comments_ngram_coll <- comments_ngram %*% 
  sparseMatrix(i = seq_len(ncol(comments_ngram)), j = match(nms, uniquenms),
               dimnames = list(nms, uniquenms), x = 1L)
comments_ngram_combined <- as.dfm(comments_ngram_coll)
head(comments_ngram_combined)
```


```{r frequency of bigrams}
# get frequency of ngrams
comments_ngram_combined_freq <- comments_ngram_combined %>%
  textstat_frequency() 

# show ten most frequent words
# part of code is taken from https://quanteda.io/reference/textstat_frequency.html
ggplot(comments_ngram_combined_freq[1:10, ], aes(x = reorder(feature, frequency), y = frequency)) +
    geom_point() +
    coord_flip() +
    labs(x = NULL, y = "Frequency")
```


```{r word-cloud of n-grams, fig.cap="Word Cloud of n-grams"}
# print word cloud, code is adapted from Lecture 4 (Part 2)
textplot_wordcloud(comments_ngram_combined, max_words=100, 
    #regulate size of the smallest and largest words               
    min_size = 1, max_size=4, 
    # words will be plotted in decreasing freq
    random_order=FALSE,
    # colors will be assigned based on freq
    random_color= FALSE, color=colors)
```

## Data Analysis

### Sentiment Analysis

dictionary-based sentiment analysis to quantify how most people felt about the video (i.e., positively or negatively)

<!-- we need to do something with sample, maybe write a function where we do at least several sampling and then average the compound for each of them -->

```{r sample data frame}
#set seed to reproduce results
set.seed(42)
#sample 1000 comments to speed up the analysis
sample_cleaned <- sample_n(cleaned, 1000)
```

VADER is one of the best unsupervised methods of analysing social media texts, because it is 'specifically attuned to sentiments expressed in social media'. We will use the compound score metric for easy evaluation of sentiment of the comments, The compound score is calculated by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalised to a value between -1 (the most extreme negative score) and +1 (the most extreme positive score).
https://github.com/cjhutto/vaderSentiment

```{r sentiment-analysis-vader, fig.cap="Histogram of compound score for sentiment analysis"}
#vade_df function
vader_results<-vader_df(sample_cleaned$textCleaned)
mean_vader<-round(mean(vader_results$compound),2)
median_vader<-round(median(vader_results$compound),2)

# following code is adapted from lecture 5
#graphical representation of compound scores
ggplot(vader_results, aes(compound)) +
  geom_histogram(bins = 10) +
  xlab("compound") +
  ylab("frequency") +
  theme_minimal() +
  #add line for mean value
  geom_vline(xintercept = c(mean_vader,median_vader), size = 1) +
  #add text for mean value
  geom_text(aes(label = paste0("Mean ", mean_vader, "; Median ", median_vader), 
                x = mean_vader, y = 10))
```

In the documentation https://github.com/cjhutto/vaderSentiment it is given that positive sentiment is when compound score >= 0.05. As we can see on \@ref(fig:sentiment-analysis-vader), the median and mean values of the compound scores for all comments are higher than 0.3, so we can assume that most of the comments are positive

<!-- I changed the tokenization process to the version we did above -->

```{r sentiment analysis dictionary}
#following code is adapted from lecture 5
#load dictionary with positive words and with negative words
poswords = "https://cssbook.net/d/positive.txt"
negwords = "https://cssbook.net/d/negative.txt"
pos = scan(poswords, what="list")
neg = scan(negwords, what="list")
sentimentdict = dictionary(list(pos=pos, neg=neg))

dic_scores = corpus(sample_cleaned$textCleaned) %>% 
  tokens() %>%
  tokens_remove(stopwords(source = 'smart')) %>%
  tokens_keep("^\\p{L}", valuetype = 'regex') %>%
  tokens_keep(min_nchar = 3) %>% 
  dfm() %>% 
  #apply dictionary to dfm
  dfm_lookup(sentimentdict) %>% 
  convert(to = "data.frame") %>% 
  mutate(sent = pos - neg)

mean_dic<-round(mean(dic_scores$sent),2)
median_dic<-round(median(dic_scores$sent),2)

#graphical representation of compound scores
ggplot(dic_scores, aes(sent)) +
  geom_histogram(bins = 10) +
  xlab("compound") +
  ylab("frequency") +
  theme_minimal() +
  #add line for mean value
  geom_vline(xintercept = c(mean_dic,median_dic), size = 1) +
  #add text for mean value
  geom_text(aes(label = paste0("Mean ", mean_dic, "; Median ", median_dic), 
                x = mean_dic, y = 10))
```

<!-- other examples:
https://dgarcia-eu.github.io/SocialDataScience/3_Affect/035_UnsupervisedToolsR/UnsupervisedToolsR.html
-->

### Sentiment Analysis with Weights

### Topic Modelling

<!-- What about low frequency words? For now, I would keep them, because they may be meaningful in the sentiment analysis. However, they are unlikely to be relevant in the topic model, so we might have to remove them then.-->

<!-- Instead of deleting low frequency words, we can apply tfidf and then to delete words or n-grams with low weights 
good example: https://quanteda.io/articles/pkgdown/replication/qss.html
-->

```{r build n-grams}
# build bigrams from tokens after lemmatisation
comments_ngram_tfidf <- comments_lemma %>% 
  tokens_remove(my_stopwords) %>% 
  tokens_ngrams(2, concatenator = " ") %>%
  dfm() %>% 
  dfm_tfidf(scheme_tf="prop", smoothing=1)
```


<!-- What about word embeddings/word2vec and n-grams?
If we decide to use word2vec, we should self-train our own model since pre-trained word embeddings tend to be biased (e.g., racist, sexist) https://smltar.com/embeddings.html#fairnessembeddings

Even self-trained models on Wikipedia and social media data are seemingly homophobic \citep{Papakyriakopoulos2020}, so maybe we shouldn't include them and justify this decision with this paper? --> 


## Robustness Checks

# Results and Discussion

# Limitations

not all comments were scraped

did not take emojis into account (merely focussed on raw text)

only considered parent comments

ambiguity of words, for example, 'love' (In the context of this study, love is a central term. However, it is also likely to be associated with a positive sentiment. In a comment to a video about aromanitcism, the term may be used to refer to the content of the video, not to express an opinion/sentiment)

# Conclusion

# Elevator Pitch


\newpage
\bibliography{references}

