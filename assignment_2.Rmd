---
title: ""
output:
  bookdown::pdf_document2:
    fig_crop: no
    fig_caption: yes
    toc: no
    number_sections: TRUE
    latex_engine: xelatex
  word_document: default
keep_tex: yes
header-includes:
- \usepackage[UKenglish]{isodate}
- \cleanlookdateon
- \usepackage{natbib}
- \bibliographystyle{agsm}
- \usepackage{hyperref}
- \usepackage{float}
- \usepackage[hang,flushmargin, bottom]{footmisc}
- \usepackage{sectsty}
- \usepackage{setspace}
- \usepackage{geometry}
- \usepackage{graphicx}
- \usepackage{fvextra}
- \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
- \newcommand\cites[1]{\citeauthor{#1}'s\ (\citeyear{#1})}


bibliography: references.bib
---

\newgeometry{top=1.8in,bottom=1.8in,right=1in,left=1in}
\spacing{1.7}

\allsectionsfont{\centering}
\subsectionfont{\raggedright}
\subsubsectionfont{\raggedright}

\pagenumbering{gobble}

\begin{centering}
  \newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
	\includegraphics[width=0.4\textwidth]{kuleuven_logo.png}\\[1cm]
	\textsc{\Large Collecting and Analyzing Big Data \\ for Social Sciences}\\[0.5cm]
	\textsc{\large B-KUL-S0K17A}\\[0.5cm]
	\HRule\\[0.6cm]
	{\huge\bfseries Assignment 2:} \\ [0.25cm]
	{\LARGE\bfseries Research Notebook}\\[0.2cm]
	\HRule\\[1.5cm]
	{\Large\emph{Group 24:}}\\[0.25cm]
	\large Ekaterina \textsc{Zaitseva} - r0870363\\
	\large Emma \textsc{Schweidler} - r0883088 \\
	\large Jade \textsc{Willaert} - r0762578\\
	\vfill\vfill\vfill
	{\large\today}
	
\end{centering}

\allsectionsfont{\raggedright}

\singlespacing

 
\newpage

\newgeometry{top = 1in, bottom = 1in, right = 1in, left = 1in}
\pagenumbering{arabic} 

```{r setup, include=FALSE}
# add space between text and code chunks
hook_source_def = knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  if (!is.null(options$vspaceecho)) {
    begin <- paste0("\\vspace{", options$vspaceecho, "}")
    stringr::str_c(begin, hook_source_def(x, options))
  } else {
    hook_source_def(x, options)
  }
})

hook_output_def = knitr::knit_hooks$get('output')
knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(options$vspaceout)) {
    end <- paste0("\\vspace{", options$vspaceout, "}")
    stringr::str_c(hook_output_def(x, options), end)
  } else {
    hook_output_def(x, options)
  }
})

# global settings
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, out.width="60%", fig.align="center",  fig.pos="H", vspaceecho='1em', vspaceout='1em')

# formatting
library(knitr)
library(formatR)
cutoff = 80
opts_chunk$set(tidy.opts = list(width.cutoff = cutoff), tidy = TRUE)
usage(head, width = cutoff)

```


# Research Question

In March of this year, the Youtube channel \citet{JaidenAnimationsVideo} uploaded a video titled ‘Being Not Straight’. The animated story, which has since received over 13 million views, is a coming out video that introduces the audience to aromanticism and asexuality\footnote{There are no agreed definitions of aromonaticism and asexuality. Common operationalisations include self-identification and the experience of little to no romantic and/or sexual attraction, desire, fantasies and/or interest \citep{Catri2021}.}. As both of these orientations are still relatively unknown, our project seeks to analyse the comment section under the video so as to investigate how the video was received. More precisely, we are interested in the audience's sentiment and the main topics discussed by the viewers.

# Methodological Design

To address our research aims, we use a combination of web scraping, sentiment analysis and topic modelling. We start our project by scraping the comments via the Youtube API. We then clean and explore the data. For this, we use a range of methods, namely subsetting, noise removal, negation bigrams, stopword removal, collocations, lemmatisation and the removal of high frequency words. After that, we conduct a dictionary-based sentiment analysis to quantify how the majority of viewers felt about the video (i.e., positively or negatively). This process consists of two steps: First, we carry out a sentiment analysis with the cleaned comments. Second, we account for any differences in the comments' popularity by weighting each comment according to their like number. Finally, to explore the key themes in the comments, we employ topic modelling.

# Data Collection

<!-- A general note on visualisations: We don't have to include plots for absolutely everything. One word cloud is enough for the report. So is one histogram for the sentiment analysis. I'd rather have fewer but well-designed and meaningful figures than plenty of messy and redundant ones. -->

In R, there are two packages for scraping Youtube comments: `vosonSML` and `tuber`. To decide which package to settle with, we retrieve the comments with both tools and compare the obtained results. At the time of data collection, that is, the 16th May 2022, \cites{JaidenAnimationsVideo} video has 231,866 comments. When scraping the comments, we receive the following data:

\begin{itemize}
  \item \emph{vosonSML}: 151,884 scraped comments
  \item \emph{tuber}: 180,743 scraped comments
\end{itemize}

For comparability purposes, we also use \cites{RiederTools} web tool to extract the comments which results in 218,848 observations. What is striking about these results is that none of the tested packages and tools are able to extract all Youtube comments. Since we conduct our analysis in R, we opt for the `tuber` package as it not only parses about 30,000 comments more than the `vosonSML` package but also is more transparent regarding issues with the number of returned comments \citep[see e.g.,][]{IssuesTuber}. The exact code we used to collect our data on the 16th May 2022 is shown below.

```{r tuber, eval = F}
# authenticate Youtube API
# (for security reasons, our personal APP_ID and APP_SECRET are not included in this script)
library(tuber)
yt_oauth("APP_ID", "APP_SECRET", token = '')

# get statistics about the video
videoid <- "qF1DTK4U1AM"
get_stats(video_id = videoid)

# scrape comments
comments <- get_all_comments(video_id = videoid)

# write output to file
write_csv(comments, "being_not_straight_16_may_22_tuber.csv")  
```

# Data Wrangling

Before starting the data cleaning process, we load all required packages in RStudio. 

```{r packages, message=F}
# load packages
library(tidyverse)
library(RColorBrewer)
library(quanteda) 
library(quanteda.textstats)
library(quanteda.textplots)
library(stopwords)
library(lexicon)
library(Matrix)
library(vader)
library(ldatuning)
library(tidytext)
library(topicmodels) 
```

Subsequent to setting up our R session, we import the collected comments.

```{r data import}
# import data
comments <- read.csv(unzip("being_not_straight_16_may_22_tuber.csv.zip","being_not_straight_16_may_22_tuber.csv"), header = T)
```

## Preliminary Cleaning

To get to know our data, we inspect the variable names.

```{r variable names}
# get variable names
print(names(comments))
```

We then follow \cites{YoutubeGenderSentiment} suggestion to remove any comments written by the content creator prior to conducting any analysis.

```{r remove creator comment}
# remove comments posted by Jaiden Animations
comments <- comments %>%
  filter(!authorChannelId.value == 'UCGwu0nbY2wSkW8N-cghnLpA')
```

Next, we create a new column indicating whether a comment is a parent comment (i.e., a main comment) or a reply (i.e., a response to another comment).

```{r boolean thread}
# create Boolean variable for thread comments
comments <- comments %>% 
  mutate(ThreadComment = if_else(is.na(parentId), FALSE, TRUE)) 
```

After that, we examine the values of the the variable \emph{moderationStatus}. This will help us to eliminate any comments marked as spam. According to \citet{YoutubeDocumentation}, valid values for this parameter are heldForReview, likelySpam, published, and rejected. 

```{r moderationStatus}
# check values of variable moderationStatus
comments %>% count(moderationStatus)
```

Given that the variable \emph{moderationStatus} contains no useful information in our case, we will drop this column when subsetting our data frame.

As we have seen above when inspecting the variable names, our data set includes 15 variables, a description of which can be found in the official \citet{YoutubeDocumentation} documentation. In our analysis, we will focus on the following four properties of the video:

\begin{itemize}
  \item \emph{textOriginal}: Unprocessed commentary text as originally published or last updated
  \item \emph{likeCount}: Number of likes on the comment
  \item \emph{parentId}: Unique identifier of the parent comment, only filled in if the comment is a response to another comment
  \item \emph{id}: Unique identifier of the comment
\end{itemize}

The decision to proceed with the variable \emph{textOriginal} rather than \emph{textDisplay} was motivated by the fact that the column \emph{textDisplay} contains the comments in html format. Consequently, html tags and entities are present in these texts which would create more work when cleaning the comments. Other variables, such as publication date or date of comment update, are also dropped from the data frame as they are irrelevant for addressing our research questions. Most importantly, when subsetting the data frame, we remove all usernames and user characteristics as we are not interested in any personal information. In doing so, we anonymise the comments. 

```{r subset columns}
# select columns
comments.cut <- comments %>% select(c(id, textOriginal, likeCount, ThreadComment))
```

Subsequent to the removal of all irrelevant columns, we filter the comments by their thread status. By only retaining parent comments, the comments that were not scraped by the `tuber` package --- which, according to \citet{IssuesTuber} and \citet{IssuesNestedComments} are likely replies in nested comments --- do not affect our research.

```{r separate data frame in parent-reply}
# filter parent comments
og.comments <- comments.cut %>% 
       filter(ThreadComment == FALSE)
```

To get a sense of how much data cleaning is needed, we now investigate the presence of special characters and non-English words in the data. We do this by computing the proportion of hashtags, urls, taggings and non-ASCII characters in the comments.

```{r hashtag-urls-taggings}
# get number of hashtags
hashtags.n <- sum(str_detect(og.comments$textOriginal, "#\\w+"))

# get number or urls
urls.n <- sum(str_detect(og.comments$textOriginal, "(http|ftp|https)://\\S+\\s*"))

# get number of taggings
mentions.n <- sum(str_detect(og.comments$textOriginal, "@\\w+"))

# get number of tweets with non-ASCII characters
nonengl.n <- sum(str_detect(og.comments$textOriginal, "[^\x01-\x7F]"))

# get number of comments
n <- dim(og.comments)[1]

# display results
cat("Share of parent comments with hashtags:", round(hashtags.n/n,3), 
    "\nShare of parent comments with urls:", round(urls.n/n,3), 
    "\nShare of parent comments with taggings:", round(mentions.n/n,3),
    "\nShare of parent comments with non-ASCII characters:", round(nonengl.n/n,3))
```

As we can see in the output, the proportion of hashtags, urls and taggings is only marginal, yet non-ASCII characters seem to make up about a fourth of our data. We will remove all special characters in the following step when removing most of the noise in the comments.

## Noise Removal and Tokenisation

Following the pre-processing of the data frame, we move on to cleaning our main variable \emph{textOriginal}. We start this process by tokenising the comments, removing punctuation, symbols and urls, converting the text to lower case and replacing contractions. We moreover use the padding argument to keep track of the position of each word. 

<!-- The following chunk does pretty much the same as the long 'noise removal' chunk we previously had (i.e., removing punctuation, urls, etc). It is not only much more concise but also has the advantage of including a padding statement which tracks the position of each word in the comments. This will come in handy when generating the n-grams later on. -->

```{r tokenisation}
contract <- c("dont", "don't", "doesn't", "isn't", "can't", "didn't")
long <- rep("not", length(contract))

comments_token <- corpus(og.comments$textOriginal) %>% 
  tokens(remove_punct = T, remove_symbols = T, remove_url = T, padding = T) %>%
  tokens_keep("^\\p{L}", valuetype = 'regex') %>%
  tokens_tolower %>%
  tokens_replace(contract, long)
```

## Negation Bigrams

To account for negations, we generate negation bigrams (i.e., bigrams that start with a negation word). We then replace the individual two words of the negation bigram by the negation bigram (e.g., 'not' and 'good' are replaced by 'not_good'). The code for this procedure is adapted from \citet{QuantedaNGrams}.

```{r negation bigrams}
# prepare negation words
neg_words <- c('no', 'not', "don't", "doesn't", "isn't", "can't", "didn't")
neg_words_star <- paste(neg_words, " *", sep = "")

# replace separated words with negative bigrams
toks_neg_bigram <- comments_token %>% tokens_compound(pattern = phrase(neg_words_star))

# show tokens of 15th comment
toks_neg_bigram[["text2"]]
```

## Stopwords

After having accounted for negation words, we can remove them along with other stopwords. For this, we use the 'smart' dictionary, because there are inconsistencies in other popular dictionaries. For example, whilst 'can't' and 'cannot' are removed with the 'en' dictionary, its counterpart 'can' is not, which would lead to skewed results. Again, we use the padding statement to remember the initial position of each word in a comment.

```{r stopwords}
# select dictionary
my_stopwords <- quanteda::stopwords(source = 'smart')

# remove stopwords
no_stopw <- toks_neg_bigram %>%
  tokens_remove(my_stopwords, padding = TRUE)

# show tokens of 15th comment
no_stopw[["text2"]]
```


## Collocations

Next, we check for the most common collocations (i.e., n-grams that occur more often than by chance). As part of this procedure, we set the minimum number of occurrences to 10 and limit the n-grams to bigrams. Due to out consistent usage of the padding statement, the function will only generate a bigram when two words were adjacent in the original comment. Our code follows the example given by \citet{QuantedaCompound}.

```{r collocations}
# remove non-words
toks_comments_cap <- no_stopw %>%
  tokens_select(pattern = "^[A-Z]",
                valuetype = "regex",
                case_insensitive = TRUE, 
                padding = TRUE)

# get collocations
tstat_col_com <- toks_comments_cap %>%
  textstat_collocations(min_count = 10, tolower = TRUE, size = 2)

# print 20 most used collocations
tstat_col_com %>% head(20) %>% as.data.frame() %>% select(c(collocation, count, z))
```

Once we have identified the collocations, we add those with a z score greater than 50 to our data. For this, a similar procedure as for the negation bigrams is used (e.g., two individual words of a collocation are replaced by the collocation). In our code implementation, we draw upon \cites{Lecture4Part2} lecture slides.

```{r replace collocations}
# filter collocations
collocations <- tstat_col_com %>%
  filter(z > 50) %>% 
  pull(collocation) %>%
  phrase()

# add filtered collocations to tokenised comments
coll <- no_stopw %>% 
  tokens_compound(collocations, join = F)

coll[['text2']]
```

## Lemmatisation

To further reduce the dimensionality of our data, we normalise the comments by means of lemmatisation. For this endeavour, we initially sought to utilize part-of-speech tagging with the `spacyr` or `udpipe` packages; however, since we encountered the same issue as the one reported by \citet{GitSpacyR} on GitHub, we could not proceed with this procedure and instead used the lemmatisation method of the `quanteda` package.

```{r lemmatisation}
# lemmatisation
comments_lemma <- coll %>%
  tokens_replace(pattern = hash_lemmas$token, replacement = hash_lemmas$lemma)

# show lemmatised tokens of second comment
comments_lemma[["text2"]]
```


## High Frequency Words

The last step of our data cleaning process revolves around the removal of high frequency words. Low frequency words are not removed at this stage as they may be valuable for the sentiment analysis. Before removing any tokens, we calculate a frequency statistic.

```{r freq stat}
# get high frequency words
comments_freq <- comments_lemma %>%  
  tokens_remove("") %>%
  dfm() %>%
  textstat_frequency()
```

Next, we visualise the word frequencies. The code for this plot is adapted from \citet{FreqPlot}.

```{r plot-frequency, fig.cap = "Word Distribution Frequencies", out.height = "70%"} 
# plot cumulative distribution frequency (CDF)
par(mfrow = c(1,2))
plot(cumsum(comments_freq$frequency)/sum(comments_freq$frequency),
     type = "l", xlab = "Number of Words", ylab = "Cumulative Distribution Frequency", 
     main = "Word CDF", col = 'red')

# plot histogram of frequencies
comments_hist <- hist(comments_freq$frequency, plot = FALSE)
plot(comments_hist$counts, ylab = "Frequency", xlab = "Word Frequency",
     main = "Word Frequency", log = "y", type = "h", col = 'red')

par(mfrow = c(1,1))

```

As we can see in Figure \@ref(fig:plot-frequency), the word frequencies are unequally distributed. In the left plot, it is apparent that the 500 most frequent words account for about 95% of our data set. In the right plot, we find that over 10,000 words appear only once in all the comments. There are also seemingly no words that appear exactly seven or eleven times.

Following this discovery, we inspect the most used words across all comments.

```{r list freq words}
# show ten most frequent words
comments_freq %>%
  head(10) %>% as.data.frame() %>% select(-group)
```

To identify words that are not meaningful to our analysis, we now go through the 100 most frequent words (output omitted due to its length). Upon identification, we remove irrelevant words from the corpus. We also drop any words with less than three characters.

```{r remove freq word}
# add words to stopword list
my_stopwords_ext <- c(my_stopwords, c(' ', 'feel', 'people', 'video', 'jaiden', 'make', 'thing', 
                                  'lot', 'year', 'part', 'stuff', 'do', 'kinda', 'yes', 'back',
                                  "she's"))
  
# remove stopwords
cleaned_comments <- comments_lemma %>% 
  tokens_remove(my_stopwords_ext) %>%
  tokens_keep(min_nchar = 3)

head(cleaned_comments)
```

Lastly, we visualise the word frequencies of the cleaned comments. This takes the form of a word cloud and is shown in Figure \@ref(fig:word-cloud). The code for this is taken from \cites{Lecture4Part2} lecture slides.

```{r word-cloud, fig.cap = "Word Cloud of Cleaned Comments"}
# generate word cloud
cleaned_comments %>% dfm() %>%
  textplot_wordcloud(max_words = 200, 
                   color = brewer.pal(8, "Dark2"),
                   random_order = FALSE,
                   min_size = 1, max_size = 4,
                   random_color = FALSE)
```
Based on the word cloud, the comments seem to be rather positive. Among the most prevalent words are the terms 'love', 'happy', 'good', 'cool', 'glad', 'nice', 'great' and 'awesome', which suggest that the viewership reacted positively. However, these preliminary findings must be approached with caution since they merely stem from a descriptive analysis.

# Data Analysis

Now that we have cleaned the comments, we can embark on the analysis. This process consists of three steps: 1) unweighted sentiment analysis, 2) weighted sentiment analysis, and 3) topic modelling.

## Unweighted Sentiment Analysis

We start our analysis with an unweighted sentiment analysis. For this method, we use two different approaches: First, we use the `vader` package, a tool specifically designed for sentiment analyses on social media data. As this package requires quite some computational power and time, we will use a random sample of 1,000 comments for this procedure. Second, we use the code underpinning the `quanteda.sentiment` package to conduct a dictionary-based sentiment analysis on the entire data set. We then compare the results of both approaches to see whether the all-purpose dictionary used on all comments is suitable for our data.

To ensure our results are reproducible, we use the set seed function. We then randomly sample 1,000 comments from our data set. As the `vader` package is applied to textual and not tokenised data, we use the uncleaned column \emph{textOriginal} for the analysis. Upon obtaining the results from the `vader` package, we calculate the average sentiment across all comments.


```{r vader}
# set seed for reproducible results
set.seed(42)

# sample 1,000 comments to speed up the analysis
sample_cleaned <- sample_n(og.comments, 1000)

# get sentiments
vader_results <- vader_df(sample_cleaned$textOriginal)

# compute mean sentiment
(mean_vader <- vader_results$compound %>% mean(na.rm = TRUE) %>% round(2))
```

Before interpreting the obtained result, we want to give a brief description of the underlying workings of the used package. According to the official documentation \citep[see][]{VaderGit}, each word in the analysed text is assigned a number, a so called \emph{valence score}, which is recorded in a dictionary. To determine a text's sentiment, all valence scores are summed and normalised. The resulting \emph{compound score} is a number between -1 (very negative) and 1 (very positive). The package creators suggest that if a compound score is equal to or greater than 0.05, the analysed text has a positive sentiment. Texts with a compound score lower than 0.05 but greater than -0.05 are said to have a neutral sentiment. A compound score lower than or equal to -0.05 indicates a negative sentiment. Based on our average compound score of 0.38 --- which is significantly above the suggested threshold of 0.05 --- it is fair to say that the overall sentiment in the comment section appears to be positive. 

To verify this finding, we carry out a dictionary-based sentiment analysis. For this, we use the underlying workings of the `quanteda.sentiment` package as described by \citet{Benoit2019} and \citet{Benoit2021}. We also use the \emph{data\_dictionary\_LSD2015} dictionary which has the clear advantage of including negations. This means that our analysis will account for our negation bigrams and recognise that terms such as 'not good' have a negative sentiment. The procedure is applied to the cleaned tokenised comments. To determine the global sentiment of the comments, we calculate the mean of the obtained sentiments scores. The code for this is taken from \citet{Benoit2019}.

```{r dict sentiment}
# apply dictionary 
toks_dict <- cleaned_comments %>% 
  tokens_lookup(dictionary = data_dictionary_LSD2015)

# document-feature matrix
dfm_dict <- toks_dict %>% dfm()

# convert to data frame
dict_output <- convert(dfm_dict, to = "data.frame")

# compute sentiment for each comment
dict_output$sent_score <- log((dict_output$positive + 
                                 dict_output$neg_negative + 0.5) / 
                                (dict_output$negative +
                                   dict_output$neg_positive + 0.5))
dict_output <- cbind(dict_output, docvars(cleaned_comments))

# show results
head(dict_output)
```

```{r mean dict sentiment}
# compute mean sentiment
(mean_dict <- dict_output$sent_score %>% mean(na.rm = TRUE) %>% round(2))
```

Akin to the previous section, we want to briefly outline how the final sentiment score is computed before making any interpretations. Whilst the official documentation does not into great detail about the sentiment calculation, \cites{Benoit2019} presentation provides useful insights into the workings of the package. According to his slides, the number of positive, negative, negative_positive (e.g., 'not good') and negative_negative (e.g., 'not dumb') words are counted per comment. The number of positive comments (i.e, the sum of the number of positive and negative_negative comments) is then divided by the number of negative comments (i.e., the sum of the number of negative and negative_positive comments). As part of this procedure, a constant is added to both denominator and numerator so as to avoid dividing by zero. Finally, the log is taken of the result to mean center the sentiment scores.

As the dictionary-based sentiment scores are not normalised to be between -1 and 1, the results are scaled somewhat differently than the ones given by the `vader` package. Nonetheless, a global sentiment score of 0.38 further supports the finding that the overall sentiment of the comments is positive.

Next, we plot the results of the dictionary-based sentiment analysis.

```{r dict-plot, fig.cap = "Histogram of Sentiment Distribution"}
# plot sentiment distribution
dict_output %>%
  ggplot(aes(sent_score)) +
  geom_histogram(bins = 20, colour = "black", fill = "white") +
  geom_density(colour = "black") +
  xlab("Sentiment Score of Dictionary Approach") +
  ylab("Frequency") +
  theme_minimal() +
  geom_vline(xintercept = mean_dict, size = 1, colour = 'red') +
  ggplot2::annotate(geom = "label", x = mean_dict + 0.9, y = Inf, 
                    label = paste0("Mean: ", mean_dict), vjust = 2, color = "red")
```
Looking at Figure \@ref(fig:dict-plot), we find that the average sentiment (red line) is greater than zero. We can further see that many comments are clustered around the zero mark, suggesting that many comments are neutral. 

As we have obtained similar results with both approaches, we will move forward with the dictionary-based sentiment analysis. The main reason for this decision is the substantial difference in execution time between both procedures. 

## Weighted Sentiment Analysis

In the next step, we will account for the number of likes of each comment. For this, we first confirm that the document-feature matrix includes the same number of observations as the data frame containing the like data. We then increment the like count by one to account for the author of a comment and multiply the result with the document-feature matrix created as part of the unweighted sentiment analysis.

```{r weights}
# check if uncleaned data and cleaned data have the same number of comments
nrow(og.comments) == nrow(dfm_dict)

# increment like count by one to avoid dropping unliked comments
weights = og.comments$likeCount + 1

# apply weights to sentiment scores of dictionary approach
dfm_dict_weighted <- dfm_dict * weights

head(dfm_dict_weighted)
```

Using the weighted document-feature matrix, we execute the same steps as above when computing the unweighted sentiment scores.

```{r mean dict sentiment weight}
# carry out same procedures as with unweighted data
dict_output_weighted <- convert(dfm_dict_weighted, to = "data.frame")

dict_output_weighted$sent_score <- log((dict_output_weighted$positive + 
                                 dict_output_weighted$neg_negative + 0.5) / 
                                (dict_output_weighted$negative +
                                   dict_output_weighted$neg_positive+ 0.5))
dict_output_weighted <- cbind(dict_output_weighted, docvars(cleaned_comments))

(mean_dict_weighted <- dict_output_weighted$sent_score %>% mean(na.rm = TRUE) %>% round(2))
```

As we can see, the weighted average sentiment score is greater than the unweighted average sentiment score. This suggests that comments with a positive sentiment have received more likes than comments with a negative sentiment. The result further supports our previous observation that the sentiment in the comment section was largely positive.

## Topic Modelling

<!-- I didn't understand what are the 'common but distinguishing features', that's why I think it's better to simply delete words that are either too rare or too frequent, I tried min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = "prop" from https://api.rpubs.com/cbpuschmann/un-stm but only 1 word left then -->

<!-- 'Common but distinguishing features' are words that occur often, but not too often, in the data. If a word is included in every comment, it doesn't tell us anything about the content of the comment - it doesn't distinguish the comments. For example, stopwords occur too often and therefore are not meaningful. Essentially, when removing 'common but distinguishing features', we remove 'words that are either too rare or too frequent'. It's just a fancy way of saying it.

There's only word left when using the code from https://api.rpubs.com/cbpuschmann/un-stm, because virtually no word occurs more than 10% in the comments after the removal of stopwords. Such cut-off value might make sense for longer texts (e.g., speeches, literature) but not for social media data. -->


<!-- there is better interpretation: https://quanteda.io/reference/dfm_trim.html -->

In addition to how the audience perceived the video in terms of emotion, we are interested to know what topics were mostly discussed in the comments. This is done using an unsupervised method - topic models. This algorithm works with a document-feature matrix, where the rows in our case are the comments and the columns (features) are the words included in the comments. Accordingly, at the intersection of row i and column j will be 1, if the word is included in the comment. Before implementing the algorithm, we will clear our DFM of rarely used words - 'features'. <!-- To do this, we use the function dfm_trim to remove words that are rare (min_docfreq = 0.1, docfreq_type = "prop") and that appear too often (max_docfreq = 0.9, docfreq_type = "prop"). [As indicated above, these thresholds do not make much sense for social media data]-->

<!-- As the description below refers to the removal of low and high frequency words, we will stick to the described cleaning procedure (I still need to paraphrase it though) -->
"we keep only the top 5% of the most frequent features (min_termfreq = 0.8) that appear in less than 10% of all documents (max_docfreq = 0.1) using dfm_trim() to focus on common but distinguishing features."

we prepare the cleaned comments for the topic modelling process

```{r topic model preparation}
# code adapted from https://tutorials.quanteda.io/machine-learning/topicmodel/

# create document-feature matrix
comments_dfm <- cleaned_comments %>% dfm()

# get vocabulary size before removing frequency words
cat("Vocabulary size before removing frequency words:", ncol(comments_dfm))

# remove frequency words
comments_dfm_trim <- comments_dfm %>% 
  dfm_trim(min_termfreq = 0.8, termfreq_type = "quantile",
           max_docfreq = 0.1, docfreq_type = "prop")

# remove empty rows
no_empty <- comments_dfm_trim %>% dfm_keep() %>% dfm_subset(ntoken(.) > 0)

# get vocabulary size after removing frequency words
cat("Vocabulary size after removing frequency words:", ncol(no_empty))
```

compute different metrics to determine the optimal number of topics

<!-- I do not recommend running the chunk below as that takes about 15 minutes. To view the results, you can import the RData file to which I saved the output. -->

```{r topic number, eval = F}
# code adapted from https://content-analysis-with-r.com/6-topic_models.html

# compute metrics for different number of topics
lda_topics <- FindTopicsNumber(no_empty, 
                                      topics = seq(from = 2, to = 15, by = 1), 
                                      metrics = c("Griffiths2004", "CaoJuan2009", 
                                                  "Arun2010", "Deveaud2014"), 
                                      method = "Gibbs", 
                                      control = list(seed = 77), 
                                      mc.cores = 2L, 
                                      verbose = TRUE)

# save output to file
save(lda_topics, file = 'ldatuning_topic_numbers.Rdata')
```



```{r topic-numbers-plot, fig.cap = "Metrics for Determining the Optimal Number of Topics"}
# import saved metrics
load("ldatuning_topic_numbers.RData")

# plot metrics
FindTopicsNumber_plot(lda_topics)
```

Figure \@ref(fig:topic-numbers-plot) suggests that three topics are a good choice for our data; however, we also need to verify this manually. Do three topics make sense semantically? Are they coherent?

<!-- The following chunk takes about 20 minutes to run. I have saved the output to an Rdata file, so you don't have to execute the code yourself. -->

```{r loop, eval = F}
# loop through different values of k
for(i in 2:10) {
  assign(paste("topicmodels_", i, sep = ""), no_empty %>% 
  convert(to = "topicmodels") %>% 
  LDA(k = i, control = list(seed = 1, alpha = 0.1)))
}

# save output to file
save(topicmodels_2, topicmodels_3, topicmodels_4, topicmodels_5, topicmodels_6, 
     topicmodels_7, topicmodels_8, topicmodels_9, topicmodels_10,
     file = 'topicmodels.Rdata')
```

```{r inspect topic models}
# import saved topic models
load("topicmodels.RData")

# print topics for different values of k
topicmodels::terms(topicmodels_2, 10)
topicmodels::terms(topicmodels_3, 10)
topicmodels::terms(topicmodels_4, 10)
topicmodels::terms(topicmodels_5, 10)
```

three topics seem to be the most coherent, though there is some overlap between topics


<!-- We still need to replace the generic topic names (i.e, Topic 1, Topic 2 , Topic 3) with more meaningful names -->

```{r plot-topic-model, fig.cap = "Keyword Probabilities per Topic", out.width="80%"}
# code adapted from https://www.tidytextmining.com/topicmodeling.html and https://juliasilge.com/blog/sherlock-holmes-stm/

# get probabilities of each word for each topic
lda_matrix <- tidy(topicmodels_4, matrix = "beta")

# get top 10 terms per topic
lda_top_terms <- lda_matrix %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

# plot word probabilities
lda_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE, width = 0.5) +
  facet_wrap(~ topic, scales = "free", labeller = labeller(topic = 
    c("1" = "Personal Experience",
      "2" = "Opinion",
      "3" = "Romance & Sexuality",
      "4" = "Support"))) +
  theme_minimal() + 
  scale_y_reordered() +
  labs(x = expression(beta),
       y = "") +
  theme(panel.spacing = unit(0.4, "lines"),
        panel.border = element_rect(color = "black", fill = NA, size = 0.4),
        strip.text = element_text(size = 11),
        plot.title = element_text(hjust = 0))

```




# Robustness Checks

validate results

randomly sample 100 comments and then manually cross-check the results of the sentiment analysis; this should be done by at least two people, so we can calculate the intercoder reliability

https://cssbook.net/chapter11.html

https://www.tandfonline.com/doi/full/10.1080/19312458.2020.1869198



For testing how accurately the algorithm detects the sentiment of comments, we will randomly select 100 comments with a computed compound_score. Each of the researchers in this paper (3 in total) will read them separately and determine their score (positive, negative, neutral) without seeing the score given by model. After that, with calculated score by model we will then assign the category for each comment (positive, negative, neutral) according to the recommendations by \citep[see][]{VaderGit} and calculate the accuracy of our model (based on confusion matrix).

<!-- Don't be afraid that doc_id has text10 and text100, it's because of automatic sorting by doc_id which is text, you can see by head(dict_output %>% arrange(doc_id))-->

```{r select 100 comments randomly with sentiments}
# merge comments and dfm matrix with sentiments
dict_output_comments <- merge(x = dict_output, y = og.comments, 
                              by = 0, all.x = TRUE)[ ,c("doc_id","textOriginal","sent_score")]

# sample 100 comments
set.seed(42)
sample_comments_sentiment <- sample_n(dict_output_comments, 100)

# delete score and save to csv file
sample_comments_sentiment_no_scores <- sample_comments_sentiment %>% 
  select(c(doc_id, textOriginal))

write.csv(sample_comments_sentiment_no_scores,"sample_sentiment_validation.csv")
```


# Results and Discussion

search for research articles on coming out videos on youtube


# Limitations

not all comments were scraped

did not take emojis into account (merely focussed on raw text)

only considered parent comments

ambiguity of words, for example, the term 'love' has different meanings depending on whether it is used as a verb or a noun. If used as a verb, it likely expresses a sentiment (e.g., 'I love your video'). If used as a noun, it probably refers to the content of the video (e.g., 'love is all around'\footnote{In the given example 'love is all around', it could be argued that the statement contains a negative sentiment, because the statement suggests that everyone experiences love when this is not necessarily the case. However, such fine-grained analysis lies far beyond the scope of this report and thus was not taken into account in our analysis.}).

speech embeddings/word2vec and part-of-speech tagging were not used

<!-- What about word embeddings/word2vec and n-grams?
If we decide to use word2vec, we should self-train our own model since pre-trained word embeddings tend to be biased (e.g., racist, sexist) https://smltar.com/embeddings.html#fairnessembeddings

Even self-trained models on Wikipedia and social media data are seemingly homophobic \citep{Papakyriakopoulos2020}, so maybe we shouldn't include them and justify this decision with this paper? --> 

# Conclusion

# Elevator Pitch


\newpage
\bibliography{references}

