---
title: ""
output:
  bookdown::pdf_document2:
    fig_crop: no
    fig_caption: yes
    toc: no
    number_sections: TRUE
    latex_engine: xelatex
  word_document: default
keep_tex: yes
header-includes:
- \usepackage[UKenglish]{isodate}
- \cleanlookdateon
- \usepackage{natbib}
- \bibliographystyle{agsm}
- \usepackage{hyperref}
- \usepackage{float}
- \usepackage[hang,flushmargin, bottom]{footmisc}
- \usepackage{sectsty}
- \usepackage{setspace}
- \usepackage{geometry}
- \usepackage{graphicx}
- \usepackage{fvextra}
- \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
- \newcommand\cites[1]{\citeauthor{#1}'s\ (\citeyear{#1})}


bibliography: references.bib
---

\newgeometry{top=1.8in,bottom=1.8in,right=1in,left=1in}
\spacing{1.7}

\allsectionsfont{\centering}
\subsectionfont{\raggedright}
\subsubsectionfont{\raggedright}

\pagenumbering{gobble}

\begin{centering}
  \newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
	\includegraphics[width=0.4\textwidth]{kuleuven_logo.png}\\[1cm]
	\textsc{\Large Collecting and Analyzing Big Data \\ for Social Sciences}\\[0.5cm]
	\textsc{\large B-KUL-S0K17A}\\[0.5cm]
	\HRule\\[0.6cm]
	{\huge\bfseries Assignment 2:} \\ [0.25cm]
	{\LARGE\bfseries Research Notebook}\\[0.2cm]
	\HRule\\[1.5cm]
	{\Large\emph{Group 24:}}\\[0.25cm]
	\large Ekaterina \textsc{Zaitseva} - r0870363\\
	\large Emma \textsc{Schweidler} - r0883088 \\
	\large Jade \textsc{Willaert} - r0762578\\
	\vfill\vfill\vfill
	{\large\today}
	
\end{centering}

\allsectionsfont{\raggedright}

\singlespacing

 
\newpage

\newgeometry{top = 1in, bottom = 1in, right = 1in, left = 1in}
\pagenumbering{arabic} 

```{r setup, include=FALSE}
# add space between text and code chunks
hook_source_def = knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  if (!is.null(options$vspaceecho)) {
    begin <- paste0("\\vspace{", options$vspaceecho, "}")
    stringr::str_c(begin, hook_source_def(x, options))
  } else {
    hook_source_def(x, options)
  }
})

hook_output_def = knitr::knit_hooks$get('output')
knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(options$vspaceout)) {
    end <- paste0("\\vspace{", options$vspaceout, "}")
    stringr::str_c(hook_output_def(x, options), end)
  } else {
    hook_output_def(x, options)
  }
})

# global settings
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, out.width="60%", fig.align="center",  fig.pos="H", vspaceecho='1em', vspaceout='1em')

# formatting
library(knitr)
library(formatR)
cutoff = 70
opts_chunk$set(tidy.opts = list(width.cutoff = cutoff), tidy = TRUE)
usage(head, width = cutoff)

```


# Research Question

In March of this year, the Youtube channel \citet{JaidenAnimationsVideo} uploaded a video titled ‘Being Not Straight’. The animated story, which has since received over 13 million views, is a coming out video that introduces the audience to aromanticism and asexuality\footnote{There are no agreed definitions of aromonaticism and asexuality. Common operationalisations include self-identification and the experience of little to no romantic and/or sexual attraction, desire, fantasies and/or interest \citep{Catri2021}.}. As both of these orientations are still relatively unknown, our project seeks to analyse the comment section under the video so as to investigate how the video was received. More precisely, we are interested in the audience's sentiment and the main topics discussed by the viewers.

# Methodological Design

To address our research aims, we use a combination of web scraping, sentiment analysis and topic modelling. We start our project by scraping the comments via the Youtube API. We then clean and explore the data. For this, we use a range of methods, namely subsetting, noise removal, negation bigrams, stopword removal, collocations, lemmatisation and the removal of unmeaningful frequency words. After that, we conduct a dictionary-based sentiment analysis to quantify how the majority of viewers felt about the video (i.e., positively or negatively). This process consists of two steps: First, we carry out a sentiment analysis with the `vader` package and a commonly used dictionary. Second, we account for any differences in the comments' popularity by weighting each comment according to their like number. Finally, to explore the key themes in the comments, we employ topic modelling.

# Data Collection

In R, there are two packages for scraping Youtube comments: `vosonSML` and `tuber`. To decide which package to use, we retrieve the comments with both tools and compare the obtained results. At the time of data collection, that is, the 16th May 2022, \cites{JaidenAnimationsVideo} video has 231,866 comments. When using the aforementioned packages, we receive the following data:

\begin{itemize}
  \item \emph{vosonSML}: 151,884 scraped comments
  \item \emph{tuber}: 180,743 scraped comments
\end{itemize}

For comparability purposes, we also use \cites{RiederTools} web tool to extract the comments, which results in 218,848 observations. What is striking about these results is that none of the tested packages and tools are able to extract all Youtube comments. Since we conduct our analysis in R, we opt for the `tuber` package as it not only parses about 30,000 comments more than the `vosonSML` package but also is more transparent regarding the issue with the number of returned comments \citep[see e.g.,][]{IssuesTuber}. The exact code we used for our data collection on the 16th May 2022 is shown below.

```{r tuber, eval = F}
# authenticate Youtube API
# (for security reasons, our personal APP_ID and APP_SECRET are not included in this script)
library(tuber)
yt_oauth("APP_ID", "APP_SECRET", token = '')

# get statistics about the video
videoid <- "qF1DTK4U1AM"
get_stats(video_id = videoid)

# scrape comments
comments <- get_all_comments(video_id = videoid)

# write output to file
write_csv(comments, "being_not_straight_16_may_22_tuber.csv")  
```

# Data Wrangling

Before starting the data cleaning process, we load all required packages in RStudio. 

```{r packages, message=F}
# load packages
library(tidyverse)
library(RColorBrewer)
library(quanteda) 
library(quanteda.textstats)
library(quanteda.textplots)
library(stopwords)
library(lexicon)
library(Matrix)
library(vader)
library(ldatuning)
library(tidytext)
library(topicmodels) 
library(irr)
```

Subsequent to setting up our R session, we import the collected comments.

```{r data import}
# import data
comments <- read.csv(unzip("being_not_straight_16_may_22_tuber.csv.zip","being_not_straight_16_may_22_tuber.csv"), header = T)
```

## Preliminary Cleaning

To get to know our data, we inspect the variable names.

```{r variable names}
# get variable names
print(names(comments))
```

We then follow \cites{YoutubeGenderSentiment} suggestion to remove any comments written by the content creator prior to conducting any analysis.

```{r remove creator comment}
# remove comments posted by Jaiden Animations
comments <- comments %>%
  filter(!authorChannelId.value == 'UCGwu0nbY2wSkW8N-cghnLpA')
```

Next, we create a new column indicating whether a comment is a parent comment (i.e., a main comment) or a reply (i.e., a response to another comment).

```{r boolean thread}
# create Boolean variable for thread comments
comments <- comments %>% 
  mutate(ThreadComment = if_else(is.na(parentId), FALSE, TRUE)) 
```

After that, we examine the values of the the variable \emph{moderationStatus}. This will help us to eliminate any comments marked as spam. According to \citet{YoutubeDocumentation}, valid values for this parameter are heldForReview, likelySpam, published, and rejected. 

```{r moderationStatus}
# check values of variable moderationStatus
comments %>% count(moderationStatus)
```

Given that the variable \emph{moderationStatus} contains no useful information in our case, we will drop this column when subsetting our data frame.

As we have seen above when inspecting the variable names, our data set includes 15 variables, a description of which can be found in the official \citet{YoutubeDocumentation} documentation. In our analysis, we will focus on the following four properties of the video:

\begin{itemize}
  \item \emph{textOriginal}: Unprocessed commentary text as originally published or last updated
  \item \emph{likeCount}: Number of likes on the comment
  \item \emph{parentId}: Unique identifier of the parent comment, only filled in if the comment is a response to another comment
  \item \emph{id}: Unique identifier of the comment
\end{itemize}

The decision to proceed with the variable \emph{textOriginal} rather than \emph{textDisplay} was motivated by the fact that the latter contains the comments in html format. Consequently, html tags and entities are present in this column, which would create more work when cleaning the comments. Other variables, such as publication date or date of comment update, are also dropped from the data frame as they are inapt for addressing our research questions. Most importantly, when subsetting the data frame, we remove all usernames and user characteristics as we are not interested in any personal information. In doing so, we anonymise the comments. 

```{r subset columns}
# select columns
comments.cut <- comments %>% select(c(id, textOriginal, likeCount, ThreadComment))
```

Subsequent to the removal of all irrelevant columns, we filter the comments by their thread status. By only retaining parent comments, the comments that were not scraped by the `tuber` package --- which, according to \citet{IssuesTuber} and \citet{IssuesNestedComments} are likely replies in nested comments --- do not affect our research.

```{r separate data frame in parent-reply}
# filter parent comments
og.comments <- comments.cut %>% 
       filter(ThreadComment == FALSE)
```

To get a sense of how much data cleaning is needed, we now investigate the presence of special characters and non-English words in the data. We do this by computing the proportion of hashtags, urls, taggings and non-ASCII characters in the comments.

```{r hashtag-urls-taggings}
# get number of hashtags
hashtags.n <- sum(str_detect(og.comments$textOriginal, "#\\w+"))

# get number or urls
urls.n <- sum(str_detect(og.comments$textOriginal, "(http|ftp|https)://\\S+\\s*"))

# get number of taggings
mentions.n <- sum(str_detect(og.comments$textOriginal, "@\\w+"))

# get number of tweets with non-ASCII characters
nonengl.n <- sum(str_detect(og.comments$textOriginal, "[^\x01-\x7F]"))

# get number of comments
n <- dim(og.comments)[1]

# display results
cat("Share of parent comments with hashtags:", round(hashtags.n/n,3), 
    "\nShare of parent comments with urls:", round(urls.n/n,3), 
    "\nShare of parent comments with taggings:", round(mentions.n/n,3),
    "\nShare of parent comments with non-ASCII characters:", round(nonengl.n/n,3))
```

As we can see in the output, the proportion of hashtags, urls and taggings is only marginal, yet non-ASCII characters seem to make up about a fourth of our data. We will remove all special characters in the following step when removing most of the noise in the comments.

## Noise Removal and Tokenisation

Following the pre-processing of the data frame, we move on to cleaning our main variable \emph{textOriginal}. We start this process by tokenising the comments, removing punctuation, symbols and urls, converting the text to lower case and replacing contractions. We moreover use the padding argument to keep track of the position of each word. 


```{r tokenisation}
contract <- c("dont", "don't", "doesn't", "isn't", "can't", "didn't")
long <- rep("not", length(contract))

comments_token <- corpus(og.comments$textOriginal) %>% 
  tokens(remove_punct = T, remove_symbols = T, remove_url = T, padding = T) %>%
  tokens_keep("^\\p{L}", valuetype = 'regex') %>%
  tokens_tolower %>%
  tokens_replace(contract, long)
```

## Negation Bigrams

To account for negations, we generate negation bigrams (i.e., bigrams that start with a negation word). We then replace the individual two words of a negation bigram by the negation bigram (e.g., ‘not' and ‘good' are replaced by ‘not_good'). The code for this procedure is adapted from \citet{QuantedaNGrams}.

```{r negation bigrams}
# prepare negation words
neg_words <- c('no', 'not', "don't", "doesn't", "isn't", "can't", "didn't")
neg_words_star <- paste(neg_words, " *", sep = "")

# replace separated words with negative bigrams
toks_neg_bigram <- comments_token %>% tokens_compound(pattern = phrase(neg_words_star))

# show tokens of 15th comment
toks_neg_bigram[["text2"]]
```

## Stopwords

After having accounted for negation words, we remove them along with other stopwords. For this, we use the ‘smart' dictionary since other popular dictionaries are inconsistent. The ‘en' dictionary, for example, removes ‘can't' and ‘cannot' but leaves the counterpart ‘can' in the data, which would lead to skewed results. To ensure expressions of appreciation are kept in the comments, we customise the stopwords by removing the words ‘thank' and ‘thanks' from the dictionary. Again, we use the padding statement to remember the initial position of each word in a comment.

```{r stopwords}
# select dictionary
my_stopwords <- quanteda::stopwords(source = 'smart')

# remove 'thank' and 'thanks' from dictionary
my_stopwords <- my_stopwords [! my_stopwords %in% c("thank", "thanks")]

# remove stopwords
no_stopw <- toks_neg_bigram %>%
  tokens_remove(my_stopwords, padding = TRUE)

# show tokens of 15th comment
no_stopw[["text2"]]
```


## Collocations

Next, we check for the most common collocations (i.e., n-grams that occur more often than by chance). As part of this procedure, we set the minimum number of occurrences to 10 and limit the n-grams to bigrams. Due to our consistent usage of the padding statement, a bigram is only generated when two words were adjacent in the original comment. Our code follows the example given by \citet{QuantedaCompound}.

```{r collocations}
# remove non-words
toks_comments_cap <- no_stopw %>%
  tokens_select(pattern = "^[A-Z]",
                valuetype = "regex",
                case_insensitive = TRUE, 
                padding = TRUE)

# get collocations
tstat_col_com <- toks_comments_cap %>%
  textstat_collocations(min_count = 10, tolower = TRUE, size = 2)

# print 20 most used collocations
tstat_col_com %>% head(20) %>% as.data.frame() %>% select(c(collocation, count, z))
```

Upon identifying the collocations, we add those with a z score of greater than 50 to our data. For this, we use a similar procedure as for the negation bigrams (i.e., two individual words of a collocation are replaced by the collocation). In our code implementation, we draw upon \cites{Lecture4Part2} lecture slides.

```{r replace collocations}
# filter collocations
collocations <- tstat_col_com %>%
  filter(z > 50) %>% 
  pull(collocation) %>%
  phrase()

# add filtered collocations to tokenised comments
coll <- no_stopw %>% 
  tokens_compound(collocations, join = F)

coll[['text2']]
```

## Lemmatisation

To further reduce the dimensionality of our data, we normalise the comments by means of lemmatisation. For this endeavour, we initially sought to utilize part-of-speech tagging with the `spacyr` or `udpipe` packages; however, since we encountered the same issue as the one reported by \citet{GitSpacyR} on GitHub, we could not proceed with this procedure and instead used the lemmatisation method of the `quanteda` package.

```{r lemmatisation}
# lemmatisation
comments_lemma <- coll %>%
  tokens_replace(pattern = hash_lemmas$token, replacement = hash_lemmas$lemma)

# show lemmatised tokens of second comment
comments_lemma[["text2"]]
```


## High Frequency Words

The last step of our data cleaning process revolves around the removal of unmeaningful words. We start this process by calculating a frequency statistic.

```{r freq stat}
# get high frequency words
comments_freq <- comments_lemma %>%  
  tokens_remove("") %>%
  dfm() %>%
  textstat_frequency()
```

Next, we visualise the computed word frequencies. The code for this is adapted from \citet{FreqPlot}.

```{r plot-frequency, fig.cap = "Word Distribution Frequencies", out.height = "70%"} 
# plot cumulative distribution frequency (CDF)
par(mfrow = c(1,2))
plot(cumsum(comments_freq$frequency)/sum(comments_freq$frequency),
     type = "l", xlab = "Number of Words", ylab = "Cumulative Distribution Frequency", 
     main = "Word CDF", col = 'red')

# plot histogram of frequencies
comments_hist <- hist(comments_freq$frequency, plot = FALSE)
plot(comments_hist$counts, ylab = "Frequency", xlab = "Word Frequency",
     main = "Word Frequency", log = "y", type = "h", col = 'red')

par(mfrow = c(1,1))

```

As we can see in Figure \@ref(fig:plot-frequency), the word frequencies are unequally distributed. In the left plot, it is apparent that the 500 most frequent words account for about 95% of our data set. In the right plot, we find that over 10,000 words appear only once in all the comments. There are also seemingly no words that appear exactly seven or eleven times.

Following this discovery, we inspect the most used words across all comments.

```{r list freq words}
# show ten most frequent words
comments_freq %>%
  head(10) %>% as.data.frame() %>% select(-group)
```

To identify words that are not meaningful to our analysis, we now go through the 100 most frequent words. The output of this procedure is omitted due to its length. Upon identification, we remove irrelevant words from the corpus. We also drop any words with less than three characters.

```{r remove freq word}
# add words to stopword list
my_stopwords_ext <- c(my_stopwords, c(' ', 'feel', 'people', 'video', 'jaiden', 'make', 'thing', 
                                  'lot', 'year', 'part', 'stuff', 'do', 'kinda', 'yes', 'back',
                                  "she's"))
  
# remove stopwords
cleaned_comments <- comments_lemma %>% 
  tokens_remove(my_stopwords_ext) %>%
  tokens_keep(min_nchar = 3)

head(cleaned_comments)
```

Lastly, we visualise the word frequencies of the cleaned comments. This takes the form of a word cloud and is shown in Figure \@ref(fig:word-cloud). The code for this is taken from \cites{Lecture4Part2} lecture slides.

```{r word-cloud, fig.cap = "Word Cloud of Cleaned Comments"}
# generate word cloud
cleaned_comments %>% dfm() %>%
  textplot_wordcloud(max_words = 200, 
                   color = brewer.pal(8, "Dark2"),
                   random_order = FALSE,
                   min_size = 1, max_size = 4,
                   random_color = FALSE)
```
Based on the word cloud, the comments seem to be rather positive. Among the most prevalent words are the terms ‘love', ‘happy', ‘good', ‘cool', ‘glad', ‘nice', ‘great' and ‘awesome', which suggest that the viewership reacted positively. However, since these observations merely stem from a descriptive analysis, these findings must be approached with caution.

# Data Analysis

Now that we have cleaned the comments, we can embark on the analysis. This process consists of three steps: (1) unweighted sentiment analysis, (2) weighted sentiment analysis, and (3) topic modelling.

## Unweighted Sentiment Analysis

We start our analysis with an unweighted sentiment analysis. For this method, we use two different approaches: First, we use the `vader` package, a tool specifically designed for sentiment analyses on social media data, to determine the sentiment of the comments. As this package requires quite some computational power and time, we use a random sample of 1,000 comments for this procedure. We opt for random sampling without replacement instead of another sampling method (e.g., systematic or stratified sampling) due to a lack of available user information\footnote{For ethical reasons, all user information was removed when pre-processing the data.}. Second, we use the code underpinning the `quanteda.sentiment` package to conduct a dictionary-based sentiment analysis on the entire data set. We then compare the results of both approaches to see whether the all-purpose dictionary used on all comments is suitable for Youtube comments.

To ensure our results are reproducible, we use the set seed function. We then randomly sample 1,000 comments from our data set. As the `vader` package is applied to textual and not tokenised data, we use the uncleaned column \emph{textOriginal} for the analysis. Upon obtaining the results from the `vader` package, we calculate the average sentiment across all comments.


```{r vader}
# set seed for reproducible results
set.seed(42)

# sample 1,000 comments to speed up the analysis
sample_cleaned <- sample_n(og.comments, 1000)

# get sentiments
vader_results <- vader_df(sample_cleaned$textOriginal)

# compute mean sentiment
(mean_vader <- vader_results$compound %>% mean(na.rm = TRUE) %>% round(2))
```

Before interpreting the obtained result, it is crucial to understand the underlying workings of the used package. According to the official documentation \citep[see][]{VaderGit}, each word in the analysed text is assigned a number, a so called \emph{valence score}, which is recorded in a dictionary. To determine a text's sentiment, all valence scores are summed and normalised. The resulting \emph{compound score} is a number between -1 (very negative) and 1 (very positive). The package creators suggest that if a compound score is equal to or greater than 0.05, the analysed text has a positive sentiment. Texts with a compound score lower than 0.05 but greater than -0.05 are said to have a neutral sentiment. A compound score lower than or equal to -0.05 indicates a negative sentiment. Based on our average compound score of 0.38 --- which is significantly above the suggested threshold of 0.05 --- the overall sentiment in the comment section appears to be positive. 

To verify this finding, we carry out a dictionary-based sentiment analysis. For this, we use the underlying workings of the `quanteda.sentiment` package as described by \citet{Benoit2019, Benoit2021}. As we are not aware of any sentiment dictionary dedicated to social media data, we make a few adaptions to the general \emph{data\_dictionary\_LSD2015} dictionary. We remove, for example, the term \emph{demis*} from the list with negative word patterns as demisexuality\footnote{Demisexuality is an identity falling under the asexual umbrella \citep{Antonsen2020}.} would be allocated a negative sentiment otherwise. However, despite the need for alterations, the used dictionary has a clear advantage: It includes negations. This means that our analysis will account for our negation bigrams and recognise that terms such as ‘not good' have a negative sentiment. 

The dictionary-based sentiment analysis is applied to the cleaned tokenised comments. To determine the global sentiment of the data, we calculate the mean of the obtained sentiments scores. The code for this is taken from \citet{Benoit2019}.

```{r dict sentiment}
my_dictionary <- data_dictionary_LSD2015
my_dictionary$negative <- my_dictionary$negative [! my_dictionary$negative %in% c("crush*", "slay*", "demis*", "damn*")]

# apply dictionary 
toks_dict <- cleaned_comments %>% 
  tokens_lookup(dictionary = my_dictionary)

# document-feature matrix
dfm_dict <- toks_dict %>% dfm()

# convert to data frame
dict_output <- convert(dfm_dict, to = "data.frame")

# compute sentiment for each comment
dict_output$sent_score <- log((dict_output$positive + 
                                 dict_output$neg_negative + 0.5) / 
                                (dict_output$negative +
                                   dict_output$neg_positive + 0.5))
dict_output <- cbind(dict_output, docvars(cleaned_comments))

# show results
head(dict_output)
```

```{r mean dict sentiment}
# compute mean sentiment
(mean_dict <- dict_output$sent_score %>% mean(na.rm = TRUE) %>% round(2))
```

Akin to the previous section, we want to briefly outline how the final sentiment score is computed before making any interpretations. Whilst the official documentation does not go into great detail about the sentiment calculation, \cites{Benoit2019} presentation provides useful insights into the workings of the package. According to his slides, the number of positive, negative, negative_positive (e.g., ‘not good') and negative_negative (e.g., ‘not dumb') words are counted per comment. The number of positive comments (i.e, the sum of the number of positive and negative_negative comments) is then divided by the number of negative comments (i.e., the sum of the number of negative and negative_positive comments). As part of this procedure, a constant is added to both denominator and numerator so as to avoid dividing by zero. Finally, the log is taken of the result to mean center the sentiment scores.

As the dictionary-based sentiment scores are not normalised to be between -1 and 1, the results are scaled somewhat differently than the ones given by the `vader` package. Nonetheless, a global sentiment score of 0.5 likewise suggests that the overall sentiment of the comments is positive. Following the calculation of the sentiment scores, we plot the results of the dictionary-based sentiment analysis.

```{r dict-plot, fig.cap = "Histogram of Sentiment Distribution"}
# plot sentiment distribution
dict_output %>%
  ggplot(aes(sent_score)) +
  geom_histogram(bins = 20, colour = "black", fill = "white") +
  geom_density(colour = "black") +
  xlab("Sentiment Score of Dictionary Approach") +
  ylab("Frequency") +
  theme_minimal() +
  geom_vline(xintercept = mean_dict, size = 1, colour = 'red') +
  ggplot2::annotate(geom = "label", x = mean_dict + 0.9, y = Inf, 
                    label = paste0("Mean: ", mean_dict), vjust = 2, color = "red")
```
Looking at Figure \@ref(fig:dict-plot), we find that the average sentiment (red line) is greater than zero. We can further see that many comments are clustered around the zero mark, suggesting that many comments are neutral. As we have obtained similar results with the `vader` package and the dictionary approach, we will move forward with the dictionary-based sentiment analysis. The main reason for this decision is the substantial difference in execution time between both procedures. 

## Weighted Sentiment Analysis

Following the unweighted sentiment analysis, we now account for the number of likes of each comment. For this, we first confirm that the document-feature matrix includes the same number of observations as the data frame containing the like data. We then increment the like count by one to account for the author of a comment. The result of this is subsequently multiplied with the document-feature matrix that we created as part of the unweighted sentiment analysis.

```{r weights}
# check if uncleaned data and cleaned data have the same number of comments
nrow(og.comments) == nrow(dfm_dict)

# increment like count by one to avoid dropping unliked comments
weights = og.comments$likeCount + 1

# apply weights to sentiment scores of dictionary approach
dfm_dict_weighted <- dfm_dict * weights

head(dfm_dict_weighted)
```

Using the weighted document-feature matrix, we execute the same steps as above when computing the unweighted sentiment scores.

```{r mean dict sentiment weight}
# carry out same procedures as with unweighted data
dict_output_weighted <- convert(dfm_dict_weighted, to = "data.frame")

dict_output_weighted$sent_score <- log((dict_output_weighted$positive + 
                                 dict_output_weighted$neg_negative + 0.5) / 
                                (dict_output_weighted$negative +
                                   dict_output_weighted$neg_positive+ 0.5))
dict_output_weighted <- cbind(dict_output_weighted, docvars(cleaned_comments))

(mean_dict_weighted <- dict_output_weighted$sent_score %>% mean(na.rm = TRUE) %>% round(2))
```

As we can see above, the weighted average sentiment score is greater than the unweighted average sentiment score. This suggests that comments with a positive sentiment have received more likes than comments with a negative sentiment. The result further supports our previous observation that the sentiment in the comment section is largely positive.

## Topic Modelling

Aside from investigating how the audience perceived the video emotionally, we seek to understand what topics prevailed in the comment section. For this, we use the unsupervised method topic modelling. To prepare the comments for the analysis, we further wrangle the already cleaned comments. This includes the creation of a document-feature matrix (DFM) and the removal of low and high frequency words. More precisely, we retain only the top 20% of the most frequent words and remove all words that appear in more than 10% of all comments. We eventually remove all empty rows as these would create issues when building the topic model. The code for this procedure is adapted from \citet{QuantedaTopicmodel}.

```{r topic model preparation}
# create document-feature matrix
comments_dfm <- cleaned_comments %>% dfm()

# get vocabulary size before removing frequency words
cat("Vocabulary size before removing frequency words:", ncol(comments_dfm))

# remove frequency words
comments_dfm_trim <- comments_dfm %>% 
  dfm_trim(min_termfreq = 0.8, termfreq_type = "quantile",
           max_docfreq = 0.1, docfreq_type = "prop")

# remove empty rows
no_empty <- comments_dfm_trim %>% dfm_keep() %>% dfm_subset(ntoken(.) > 0)

# get vocabulary size after removing frequency words
cat("Vocabulary size after removing frequency words:", ncol(no_empty))
```

Prior to fitting the model, we determine the parameter $k$, that is, the number of topics in the comments. To this end, we compute different metrics to help us identify the optimal number of topics. The code for this is taken from \citet{Puschmann2022}.

<!-- The following chunk takes about about 15 minutes to run. -->

```{r topic number, eval = F}
# compute metrics for different number of topics
lda_topics <- FindTopicsNumber(no_empty, 
                                      topics = seq(from = 2, to = 15, by = 1), 
                                      metrics = c("Griffiths2004", "CaoJuan2009", 
                                                  "Arun2010", "Deveaud2014"), 
                                      method = "Gibbs", 
                                      control = list(seed = 77), 
                                      mc.cores = 2L, 
                                      verbose = TRUE)

# save output to file
save(lda_topics, file = 'ldatuning_topic_numbers.Rdata')
```

```{r load saved metrics, echo = F}
# import saved metrics
load("ldatuning_topic_numbers.RData")
```

```{r topic-numbers-plot, fig.cap = "Metrics for Determining the Optimal Number of Topics"}
# plot metrics
FindTopicsNumber_plot(lda_topics)
```

According to Figure \@ref(fig:topic-numbers-plot), there is no straightforward value for $k$. Whereas the metrics based on Griffiths2004 and Arun2010 gradually improve and thus support $k = 15$, CaoJuan2009 and Deveaud2014's measures suggest that $k = 3$ and $k = 10$ fit our data well. Given these inconclusive results, we now fit various topic models with different numbers for $k$ (range 2 to 10). 

<!-- The following chunk takes about 20 minutes to run. -->

```{r loop, eval = F}
# loop through different values of k
for(i in 2:10) {
  assign(paste("topicmodels_", i, sep = ""), no_empty %>% 
  convert(to = "topicmodels") %>% 
  LDA(k = i, control = list(seed = 1, alpha = 0.1)))
}

# save output to file
save(topicmodels_2, topicmodels_3, topicmodels_4, topicmodels_5, topicmodels_6, 
     topicmodels_7, topicmodels_8, topicmodels_9, topicmodels_10,
     file = 'topicmodels.Rdata')
```

Next, we look at the output of the different topic models to manually check which number of topics makes most sense semantically. Due to the length of the output, we only display models with $k <= 6$ in this report. 

```{r load saved topic models, echo = F}
# import saved topic models
load("topicmodels.RData")
```


```{r inspect topic models}
# print topics for different values of k
topicmodels::terms(topicmodels_2, 10)
topicmodels::terms(topicmodels_3, 10)
topicmodels::terms(topicmodels_4, 10)
topicmodels::terms(topicmodels_5, 10)
topicmodels::terms(topicmodels_6, 10)
```

Based on the output of the different models, four topics seem to be the most coherent, although it should be noted that the topics are not clearly distinguished in any of the inspected models. It is further apparent that with a growing number of topics, the overlap in the models increases. Upon identifying four as the value for $k$, we assign preliminary names to each topic and plot the keywords and respective probabilities. The code for the visualisation is adopted from \citet{Silge2018} and \citet{Silge2022}.


```{r plot-topic-model, fig.cap = "Keyword Probabilities per Topic", out.width="80%"}
# get probabilities of each word for each topic
lda_matrix <- tidy(topicmodels_4, matrix = "beta")

# get top 10 terms per topic
lda_top_terms <- lda_matrix %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

# plot word probabilities
lda_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE, width = 0.5) +
  facet_wrap(~ topic, scales = "free", labeller = labeller(topic = 
    c("1" = "Personal Experiences",
      "2" = "Opinion",
      "3" = "Romance & Sexuality",
      "4" = "Support"))) +
  theme_minimal() + 
  scale_y_reordered() +
  labs(x = expression(beta),
       y = "") +
  theme(panel.spacing = unit(0.4, "lines"),
        panel.border = element_rect(color = "black", fill = NA, size = 0.4),
        strip.text = element_text(size = 11),
        plot.title = element_text(hjust = 0))

```

# Robustness Checks

Following our analyses, we now validate the obtained results.

## Sentiment Analyis Validation

We begin the validation process by testing how accurately the dictionary approach detects the sentiment of the comments. For this, we loosely follow the procedure outlined by \citet{SentimentValidation}. 

First, we randomly sample 100 comments. Each of the researchers in this paper, that is, three in total, then reads the sampled comments and classifies them as either positive, negative or neutral. When coding the comments, the researchers are unaware of the results of the dictionary-based sentiment analysis.

```{r sample comments}
# merge comments and dfm matrix with sentiments
dict_output_comments <- merge(x = dict_output, y = og.comments, 
                              by = 0, 
                              all.x = TRUE)[ , c("doc_id", "textOriginal", "sent_score")]

# sample 100 comments
set.seed(42)
sample_comments_sentiment <- sample_n(dict_output_comments, 100)

# delete score and save to csv file
sample_comments_sentiment_no_scores <- sample_comments_sentiment %>% 
  select(c(doc_id, textOriginal))

# write table to file
write.csv(sample_comments_sentiment_no_scores,"sample_sentiment_validation.csv")
```

Upon manually annotating the data, we import the coded comments and merge them into a data frame.

```{r import manual coding sent}
# import manually coded comments
coder_1 <- read_csv("sentiment_validation_emma.csv", col_select = c("doc_id", "sentiment_emma"))
coder_2 <- read_csv("sentiment_validation_jade.csv", col_select = c("doc_id", "sentiment_jade"))
coder_3 <- read.csv("sample_validation_kate.csv", header = T, sep = ";") %>% select("doc_id", "sentiment_kate")

# merge data frames
sentiment_validation <- merge(coder_1, coder_2, by = "doc_id") %>% merge(coder_3)

# show merged data frame
head(sentiment_validation)
```

We then transpose the data frame and subsequently calculate the intercoder reliability using Krippendorff's $\alpha$. 

<!-- The warning message "NAs introduced by coercion" can be ignored: https://stackoverflow.com/questions/21907540/irr-krippendorfs-alpha-with-non-numeric-classifications-warning-message -->
```{r intercoder reliability}
# transform data frame
sentiment_validation_t <- sentiment_validation %>% t() %>% as.data.frame()
names(sentiment_validation_t) <- sentiment_validation_t[1, ] %>% as.matrix()
sentiment_validation_t <- sentiment_validation_t[-1, ]

# compute intercoder reliability
sentiment_validation_t %>% as.matrix() %>% kripp.alpha(method = "ordinal")

# compute pairwise intercoder reliability
sentiment_validation_t[1:2, ] %>% as.matrix() %>% kripp.alpha(method = "ordinal")
sentiment_validation_t[2:3, ] %>% as.matrix() %>% kripp.alpha(method = "ordinal")
sentiment_validation_t[-2, ] %>% as.matrix() %>% kripp.alpha(method = "ordinal")
```

As we can see above, Krippendorff’s $\alpha$ is 0.77, with the pairwise intercoder reliability ranging from 0.69 to 0.87. Using a majority vote, we now add a new column to the data frame, which indicates the dominant sentiment from the manual coding.

```{r dominant categries}
# determine dominant categories among manual coding
sentiment_validation_t <- sentiment_validation_t %>% 
  apply(2,function(x) names(which.max(table(x)))) %>%
  rbind(sentiment_validation_t, sentiment = .)

# show updated data frame
sentiment_validation_t[, 1:6]
```

Next, in accordance with the recommendations made by the creators of the `vader` package \citep{VaderGit}, each numerical sentiment score from the unweighted sentiment analysis is assigned one of the following sentiment categories: positive, negative or neutral. 


```{r recode sent output}
# re-code output of sentiment analysis
sentiment_validation_results <- sample_comments_sentiment %>% select(-textOriginal) %>%
  mutate(sentiment = ifelse(sent_score >= 0.05, "positive", 
                           ifelse(sent_score <= -0.05, "negative", "neutral")))

# show re-coded data frame
head(sentiment_validation_results)
```
After that, we transpose the data frame with the results of the dictionary approach and merge it with the dominant results from the manual coding.

```{r merge sentiments}
# transform dict data frame
sentiment_validation_results_t <- sentiment_validation_results %>% select(-sent_score) %>%
  t() %>% as.data.frame()
names(sentiment_validation_results_t) <- sentiment_validation_results_t[1, ] %>% as.matrix()
sentiment_validation_results_t <- sentiment_validation_results_t[-1, ]

# merge manual and dictionary sentiments
sentiment_merged <- rbind(sentiment_manual = sentiment_validation_t[4, ], 
      sentiment_dictionary = sentiment_validation_results_t)

# show merged sentiments
sentiment_merged[, 1:5]
```
Finally, we calculate Krippendorff's $\alpha$ to check the level of agreement between the manually coded sentiments and the dictionary-based sentiments.

```{r agreement dict man}
# compute agreement between manual coding and dictionary approach
sentiment_merged %>% as.matrix() %>% kripp.alpha(method = "ordinal")
```

As is apparent in the output, Krippendorff's $\alpha$ is 0.37, which is rather low but higher than the values obtained by \citet{SentimentValidation} when comparing the precision of different dictionaries. To investigate any misclassifications, we now take a look at a confusion matrix.

```{r confusion matrix}
# create confusion matrix
sentiment_merged_t <- t(sentiment_merged) %>% as.data.frame
table(DICTIONARY = sentiment_merged_t$sentiment_dictionary,
      MANUAL = sentiment_merged_t$sentiment_manual)
```

The table reveals that most comments (i.e., 59) are correctly classified. We can further see that when misclassifications occur, this mostly tends to happen between neutral and positive sentiments, with the dictionary seemingly underpredicting the number of positive sentiments. What stands out in the table is that two comments are classified as positive even though the coders deem them negative, and two comments are classified as negative even though the coders deem them positive. To further investigate this, we take a closer look at the misclassified comments. Although this procedure is undertaken for all misclassified comments, we exemplify our course of action with comment number 97,832.

```{r inspection}
# show full comment
og.comments$textOriginal[[97832]]

# show cleaned comment
cleaned_comments[97832, ]

# show sentiment score
dict_output[97832, ]
```

As shown above, the dictionary-based approach assigns comment number 97,832 a positive sentiment. This is because the terms \emph{love} and \emph{uphold*} are included as positive word patterns in the used dictionary. The manual coders, conversely, classify the comment as negative\footnote{To be precise, two researchers coded it as negative, and one coded it as neutral. The overall manual sentiment was therefore negative.}, because it invalidates Jaiden's experiences and is generally condescending. Yet, although the scores assigned by the dictionary approach contradict the manually coded sentiments, the inspection of the word patterns suggests that this is not a result of a poorly constructed dictionary but more so an issue of the employed method which largely neglects the context of words.

## Topic Model Validation

As our analysis is exploratory and not confirmatory, it is somewhat difficult to assess the fit of our topic model. Whilst goodness-of-fit measures, such as perplexity and coherence values, may inform the model building process, we refrain from computing such metrics at the validation stage since they have been shown to not be very illuminating with regard to the interpretability of topics \citep{perplexity}. Instead, we follow \cites{CSSBook} suggestion to manually assess the topic coherence by looking at the top keywords and documents of each topic. In doing so, we seek to deepen our understanding of the identified topics and reconsider the initial topic names. Hereby, we focus on two aspects in particular: \emph{face validity} and \emph{semantic validity} \citep[as described by][]{Krippendorff2004}. 

To do this, we filter the topic model results by $\gamma$. We then merge the data frame with the column \emph{textOriginal} and only keep the top 50 comments per topic. Finally, we write the output to a csv file to allow the three researchers in this project to manually evaluate the four topics.

```{r top documents}
# get topic proportions for each comment
topic_comment <- tidy(topicmodels_4, matrix = "gamma") %>%
  mutate(id = as.numeric(gsub(".*?([0-9]+).*", "\\1", .$document)))

# get top 50 comments per topic
tm_validation <- topic_comment[FALSE,]
for (i in 1:4) {
  tm_validation <- rbind(tm_validation, assign(paste("topic_", i, sep = ""), topic_comment %>% 
           filter(topic == i) %>% arrange(-gamma) %>% head(50)))
}

# merge with full comment
tm_validation_all <- merge(x = tm_validation, og.comments, by.x = "id", by.y = 0, all.y = F) %>% select(-c(likeCount, ThreadComment, id.y, id)) %>% arrange(topic)

# write table to file
write.csv(tm_validation_all, "topic_modelling_validation.csv")
```

Upon close inspection of the topics, we qualitatively compare our results. In the case of the first topic, which we initally labelled \emph{Personal Experience}, all coders agree that commenters share their own experiences, mostly in relation to crushes. As coders 1 and 2 note, many commenters relate to the ‘crush experience' mentioned in \cites{JaidenAnimationsVideo} video, which essentially means they purposely decided to have a crush at some point in their life without actually having had one. Coder 3, similarly, reports that commenters ‘relate', ‘realise' and give ‘examples'. Seemingly, the described experience is not only shared by people identifying under the aromantic asexual umbrella, but also by other LGBT+ folk.

As for the second topic, which we initially assigned the name \emph{Opinion}, there is a consensus among coders that all comments scoring high on this topic can be classified as spam. It is moreover undisputed that the topic contains unrelated messages (e.g., Christian and Islamic prayers), trolling (e.g., about 25 comments repeat ‘not being straight is bad’ over and over again), and non-English comments (mostly Spanish). This is a particularly interesting finding given that the keywords suggested a completely different topic interpretation. In light of these results, we change the topic name to \emph{Spam}.

In terms of the third topic, which we initially named \emph{Romance and Sexuality}, the coders agree that the topic is less well defined than the other three. They further concur that the comments include some personal experiences and are not entirely positive. Aside from these concurrences, the coders have slightly different --- but overlapping --- interpretations of the topic. Coder 3, for example, observes that the top comments are "in the spirit of a debate". This is akin to coder 2's summary that commenters are "lecturing her [Jaiden Animations] on sexuality/biology/psychology". Coder 1, similarly, remarks that the comments include theorisations, explanations, medicalisations, and critcisms of asexuality, aromanticism and other related identities. We therefore decide to re-name this topic to \emph{Opinion}.

With respect to the fourth topic, which we initially titled \emph{Support}, the researchers
agree that the comments are positive, supportive and thankful for the video. Coders 1 and 2 further note that the commenters discuss the relevance of the video and its importance for the LGBT+ community. Given these expressions of appreciation, we update the topic name to \emph{Praise}. A synopsis of our validation results is presented in Table \ref{Results of Topic Model Validation} below.

\vspace{15pt}
\begingroup
\setlength{\tabcolsep}{10pt} 
\renewcommand{\arraystretch}{1.5} 
\begin{table}[H]
\centering
\begin{tabular}{ l l }
 \hline
  \textbf{Initial Topic Names} & \textbf{Final Topic Names} \\ 
 \hline
 Personal Experience & Personal Experience \\
 Opinion & Spam \\
 Romance \& Sexuality & Opinion \\
 Support & Praise \\
 \hline
\end{tabular}
\caption{Topic Names Pre- and Post-Validation}
\label{Results of Topic Model Validation}
\end{table}
\endgroup
\vspace{10pt}

# Results and Discussion

One of the main objectives of our study is to understand the comment sentiment of \cites{JaidenAnimationsVideo} video. Based on an exploratory word cloud and unweighted sentiment analyses with the `vader` package and an all-purpose dictionary, our findings suggest that the overall sentiment is positive. This is even more so the case when considering the results of the weighted sentiment analysis, which indicate that positive comments are more liked than neutral or negative comments. This result is in line \cites{Siersdorfer2010} finding that Youtube users tend to like positive comments and dislike negative comments\footnote{As of November \citeyear{Youtube2021}, \citeauthor{Youtube2021} has privated the dislike count. Although videos and comments can still be disliked, the number of dislikes is no longer publicly displayed.}.

On the question of the discussed topics in the comments, our analysis found that many commenters share their own experiences, post unrelated messages, express their opinion about the video and its content, and show support and appreciation for the creator. In light of \cites{Fox2016} research findings, which show that "online role models [are] most essential for individuals with identities that are rarely portrayed or invisible in regular media, such as asexuals" (p. 641), the prevalence of praise and support in the comments is not surprising. Similarly, the emphasis on the relevance and importance of the video for LGBT+ people was somewhat expected given that aromanticism and asexuality are still relatively unknown. This lack of awareness is well illustrated by the fact that the first academic article focussing exclusively on aromantic-asexual people was only published fairly recently by \citeauthor{Antonsen2020} in \citeyear{Antonsen2020}. 

With regard to criticism and opinions, an important finding was that a diverse range of views was voiced by the commenters. According to \cites{Thelwall2012} general research on Youtube comments, debates seemingly occur more frequently in videos with many comments. Consequently, it was anticipated that some form of discourse would emerge in the analysed data. This notion was further strengthened by the fact that the video thumbnail, which is shown Figure \ref{fig:thumbnail}, does not indicate that the video is about aromanticism and asexuality. As a result, some viewers might have expected a different type of content and thus were taken by surprise by the focus of the video. In fact, the video creator Jaiden Animations even jokes about this in the beginning of the video by saying "I've come to realise that I'm aro ace, which stands for aromantic asexual --- and I know what you're thinking: That's not gay, what the hell is that?" (0:45-0:55). This links back to our previous finding that aromanticism and asexuality are still largely invisible.

\vspace{15pt}
\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{thumbnail.jpg}
\caption{Thumbnail of \cites{JaidenAnimationsVideo} Video}
\label{fig:thumbnail}
\end{figure}


# Limitations

A number of limitations need to be noted regarding our project. First, we were unable to scrape all comments via the Youtube API, wherefore we limited our analysis to parent comments. Consequently, it is possible that our findings do not represent the overall sentiment expressed and topics dicussed by the commenters. Second, our data cleaning process did not remove all non-English and/or spam comments. This not only resulted in one of the topics being largely unmeaningful but also potentially impacted the results of the sentiment analysis. However, since we have found strong evidence for an overall positive sentiment and have validated these results manually, it is unlikely that the spam comments have distorted our findings. Third, we did not account for the ambiguity of words. The term ‘love', for example, has different meanings depending on whether it is used as a verb or a noun. If used as a verb, it is likely to express a sentiment (e.g., ‘I love your video'), whereas if used as a noun, it probably refers to the content of the video (e.g., ‘love is all around'\footnote{In the given example ‘love is all around', it could be argued that the statement contains a negative sentiment as the statement suggests that everyone experiences love when this is not necessarily the case. However, such fine-grained analysis lies far beyond the scope of this report and thus was not taken into account in our analysis.}). Although this issue could have been addressed by using part-of-speech tagging, we did not employ this method for reasons discussed in section \@ref(lemmatisation). Other sophisticated techniques, such as word2vec, were also not used because pre-trained word embeddings tend to be biased (e.g., racist, sexist) \citep{Hvitfeldt2022} and self-trained models on social media data are seemingly homophobic \citep{Papakyriakopoulos2020}. Lastly, since our analysis merely focussed on raw text, we did not consider any emojis or other non-textual data in our analysis. The dictionary approach was further unable to capture any latent messages in the comments and disregarded the context of the made statements.

# Conclusion

This study was undertaken to analyse the comment section of \cites{JaidenAnimationsVideo} coming out video. More specifically, we sought to assess the views and opinions expressed by the commenters and to identify their main talking points. To this end, we combined an unweighted and weighted sentiment analysis framework with that of topic modelling. Overall, all of our analyses indicated a general positive sentiment of the comments. The results further provided insights for the discussed topics. Most importantly, we found that the comments can be clustered into four thematic groups. These include shared experiences, unrelated ideas, general thoughts on romance and sexuality, and support and gratitude.

# Elevator Pitch

Our project set out to explore how an aromantic asexual coming out story was received and conferred in the video's comment section. To address our research aims, we employed a sentiment analysis approach (unweighted and weighted by like numbers) as well as topic modelling. Our findings suggest that the video was received mostly positively, with the main points of discussion being related to personal experiences, spam, opinions, and praise.

\newpage
\bibliography{references}

